{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0bb814",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "A continuación se muestra una descripción general de alto nivel del sistema que queremos construir:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dff98a",
   "metadata": {},
   "source": [
    "<img src='images/img_1.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd650181",
   "metadata": {},
   "source": [
    "# PARTE I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad2438",
   "metadata": {},
   "source": [
    "Empecemos cargando las variables de entorno que necesitamos utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35475b",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Definamos el modelo LLM que utilizaremos como parte del flujo de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22ed43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Este es el video de YouTube que vamos a utilizar.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=qcJM0bM3D0Q'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef919e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configurar el modelo de Gemini\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')  # Usa el modelo que prefieras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e950151",
   "metadata": {},
   "source": [
    "Probamos el modelo haciendo una pregunta sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24d834a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de Francia es París.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar el modelo con una pregunta sencilla\n",
    "pregunta_sencilla = \"¿Cuál es la capital de Francia?\"\n",
    "respuesta = model.generate_content(pregunta_sencilla)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(respuesta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c2fea",
   "metadata": {},
   "source": [
    "El resultado del modelo es una instancia de `AIMessage` que contiene la respuesta. Podemos extraer esta respuesta encadenando el modelo con un analizador de salida [outputParser](https://python.langchain.com/docs/modules/model_io/output_parsers/).\n",
    "\n",
    "Así es como se ve el encadenamiento del modelo con un analizador de salida:\n",
    "\n",
    "<img src='images/chain1.png' width=\"1200\">\n",
    "\n",
    "Para este ejemplo, utilizaremos un `StrOutputParser` simple para extraer la respuesta como una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0beeb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: La capital de Alemania es Berlín.\n",
      "\n",
      "Tipo de respuesta: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def gemini_invoke(input_text):\n",
    "    # Verificar si input_text es un diccionario de LangChain\n",
    "    if isinstance(input_text, dict) and \"messages\" in input_text:\n",
    "        # Extraer el contenido del mensaje del usuario\n",
    "        messages = input_text[\"messages\"]\n",
    "        if messages and hasattr(messages[0], \"content\"):\n",
    "            input_text = messages[0].content\n",
    "\n",
    "    # Asegurar que el input sea string\n",
    "    if not isinstance(input_text, str):\n",
    "        input_text = str(input_text)\n",
    "\n",
    "    response = model.generate_content(input_text)  \n",
    "    return response.text\n",
    "\n",
    "\n",
    "# Creamos un analizador de salida (StrOutputParser) para asegurarnos de que la respuesta sea una cadena\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Creamos la cadena combinando el modelo y el parser\n",
    "chain = gemini_invoke | parser\n",
    "\n",
    "# Probamos la cadena con una pregunta sencilla\n",
    "pregunta_sencilla = \"¿Cuál es la capital de Alemania?\"\n",
    "respuesta_parseada = chain.invoke(pregunta_sencilla)\n",
    "\n",
    "# Imprimimos la respuesta (ahora debería ser una cadena directamente)\n",
    "print(\"Respuesta:\", respuesta_parseada)\n",
    "print(\"Tipo de respuesta:\", type(respuesta_parseada))  # Para verificar que es un string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e848d0",
   "metadata": {},
   "source": [
    "## Presentamos las plantillas de preguntas\n",
    "\n",
    "Queremos contextualizar el modelo y la pregunta. [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) Son una forma sencilla de definir y reutilizar indicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa35155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: París\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Definir la plantilla de pregunta\n",
    "template = \"\"\"\n",
    "Responda la pregunta según el contexto descrito a continuación. Si no puede responder, responda \"No lo sé\".\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\"\"\"\n",
    "\n",
    "# Crear la plantilla de prompt\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Probar la plantilla con un ejemplo\n",
    "contexto_ejemplo = \"París es la capital de Francia y una de las ciudades más visitadas del mundo.\"\n",
    "pregunta_ejemplo = \"¿Cuál es la capital de Francia?\"\n",
    "\n",
    "# Formatear el prompt con los valores de contexto y pregunta\n",
    "formatted_prompt = prompt.format(contexto=contexto_ejemplo, pregunta=pregunta_ejemplo)\n",
    "\n",
    "# Usar el modelo de Gemini para generar una respuesta\n",
    "respuesta = model.generate_content(formatted_prompt)\n",
    "\n",
    "# Imprimir la respuesta\n",
    "print(\"Respuesta:\", respuesta.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100f1a9",
   "metadata": {},
   "source": [
    "Ahora podemos encadenar el mensaje con el modelo y el analizador de salida.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac2eb9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        contexto=RunnablePassthrough(),\n",
    "        pregunta=RunnablePassthrough()\n",
    "    )\n",
    "    | prompt\n",
    "    | RunnableLambda(gemini_invoke)\n",
    "    | parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f36b2",
   "metadata": {},
   "source": [
    "## Combinación de cadenas\n",
    "\n",
    "Podemos combinar diferentes cadenas para crear flujos de trabajo más complejos. Por ejemplo, creemos una segunda cadena que traduzca la respuesta de la primera a otro idioma.\n",
    "\n",
    "Comencemos creando una nueva plantilla de solicitud para la cadena de traducción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc59531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Traduce {answer} al {language}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276e2fb",
   "metadata": {},
   "source": [
    "Ahora podemos crear una nueva cadena de traducción que combine el resultado de la primera cadena con la solicitud de traducción.\n",
    "\n",
    "Así es como se ve el nuevo flujo de trabajo:\n",
    "\n",
    "<img src='images/chain3.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5079dc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "París\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Plantilla para asegurar respuestas directas y concisas\n",
    "qa_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Responde únicamente con la respuesta correcta basada en el contexto.\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\n",
    "Devuelve solo la respuesta sin explicaciones ni comentarios adicionales.\n",
    "\"\"\")\n",
    "\n",
    "# Función de invocación con limpieza\n",
    "def clean_gemini_invoke(inputs):\n",
    "    response = gemini_invoke(inputs)  # Llamamos a Gemini\n",
    "    if isinstance(response, dict) and \"text\" in response:\n",
    "        return response[\"text\"].strip()\n",
    "    return response.strip()\n",
    "\n",
    "# Cadena de QA\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"contexto\": RunnablePassthrough(),\n",
    "        \"pregunta\": RunnablePassthrough()\n",
    "    }\n",
    "    | qa_template\n",
    "    | RunnableLambda(clean_gemini_invoke)\n",
    ")\n",
    "\n",
    "# Plantilla de traducción simplificada\n",
    "translation_template = ChatPromptTemplate.from_template(\"\"\"\n",
    "Traduce al {language} el siguiente texto sin modificarlo:\n",
    "\n",
    "{text}\n",
    "Solo devuelve el texto traducido, sin explicaciones ni estructura JSON.\n",
    "\"\"\")\n",
    "\n",
    "# Cadena de traducción asegurando solo texto\n",
    "translation_chain = (\n",
    "    {\n",
    "        \"text\": RunnablePassthrough(),\n",
    "        \"language\": lambda _: \"Castellano\"\n",
    "    }\n",
    "    | translation_template\n",
    "    | RunnableLambda(clean_gemini_invoke)\n",
    ")\n",
    "\n",
    "# Función que encadena QA y traducción\n",
    "def qa_then_translate(inputs):\n",
    "    respuesta_qa = qa_chain.invoke(inputs)  # Obtenemos respuesta en inglés\n",
    "    return translation_chain.invoke({\"text\": respuesta_qa})  # Traducimos solo el texto\n",
    "\n",
    "# Cadena combinada con correcciones\n",
    "combined_chain = RunnableLambda(qa_then_translate)\n",
    "\n",
    "# Prueba final\n",
    "inputs = {\n",
    "    \"contexto\": \"París es la capital de Francia y una de las ciudades más visitadas del mundo.\",\n",
    "    \"pregunta\": \"¿Cuál es la capital de Francia?\"\n",
    "}\n",
    "\n",
    "respuesta_traducida = combined_chain.invoke(inputs)\n",
    "\n",
    "print(respuesta_traducida)  # Eliminamos \"Respuesta traducida:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a226673",
   "metadata": {},
   "source": [
    "# PARTE II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0366ee9",
   "metadata": {},
   "source": [
    "## Transcripcion de video de YouTube\n",
    "\n",
    "El contexto que queremos enviar al modelo proviene de un video de YouTube. Descargamos el video y transcribámoslo con [OpenAI's Whisper](https://openai.com/research/whisper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db966556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡El archivo de transcripción ya existe!\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "import os\n",
    "import yt_dlp  # Using yt-dlp instead of pytube\n",
    "\n",
    "YOUTUBE_VIDEO = 'https://www.youtube.com/watch?v=qcJM0bM3D0Q'  # Coloca tu enlace de YouTube aquí\n",
    "\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    print(f\"Descargando video: {YOUTUBE_VIDEO}\")\n",
    "    \n",
    "    # Crear un directorio temporal para la descarga\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # Opciones de yt-dlp para descargar solo el audio\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'outtmpl': os.path.join(tmpdir, 'audio.%(ext)s'),\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "            'quiet': False\n",
    "        }\n",
    "        \n",
    "        # Descargar el audio\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.extract_info(YOUTUBE_VIDEO, download=True)\n",
    "            audio_file = os.path.join(tmpdir, 'audio.mp3')\n",
    "        \n",
    "        print(f\"Transcribiendo archivo de audio: {audio_file}\")\n",
    "        \n",
    "        # Cargar el modelo Whisper\n",
    "        whisper_model = whisper.load_model(\"base\")  # Puedes usar \"small\", \"medium\", o \"large\" si prefieres más precisión\n",
    "        \n",
    "        # Transcribir el audio\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        # Guardar la transcripción en un archivo\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)\n",
    "        \n",
    "        print(\"Transcripción completada y guardada en 'transcription.txt'\")\n",
    "else:\n",
    "    print(\"¡El archivo de transcripción ya existe!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e1c9e",
   "metadata": {},
   "source": [
    "Vamos a leer la transcripción y mostrar los primeros caracteres para asegurarnos de que todo funciona como se espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09313d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Every time I think of you I feel shot right through with a ball of blue It's no problem to mine but \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7897a",
   "metadata": {},
   "source": [
    "## Usando la transcripción completa como contexto\n",
    "\n",
    "Si intentamos invocar la cadena usando la transcripción como contexto, el modelo devolverá un error porque el contexto es demasiado largo.\n",
    "\n",
    "Los modelos de lenguaje grandes admiten tamaños de contexto limitados. El vídeo que estamos usando es demasiado largo para que el modelo lo pueda procesar, por lo que necesitamos buscar una solución diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b70fd63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta de Gemini: Esta canción habla de la confusión y la frustración del amor no correspondido o de una relación difícil. El narrador está claramente afectado por alguien (\"Cada vez que pienso en ti me siento atravesado por una bola azul\") pero no puede articular sus sentimientos (\"Ves las palabras que no puedo decir\"). \n",
      "\n",
      "Hay una sensación de anhelo y nostalgia por un tiempo más simple (\"¿Por qué no podemos ser nosotros mismos como ayer?\"). El narrador se siente solo y perdido a pesar de intentar mantener una apariencia de estar bien (\"Estoy bastante simple, he estado solo\", \"Me siento bien y me siento bien\").\n",
      "\n",
      "La repetición de \"Cada vez que veo caer\" junto con la sensación de soledad (\"He estado solo\") sugiere una vulnerabilidad y una sensación de estar perdiendo el control o de \"caer\" en el amor o en la desesperación.  La \"sabiduría del tonto\" que no liberará al narrador podría referirse a consejos bienintencionados pero inútiles o a la propia incapacidad del narrador para razonar con sus sentimientos.\n",
      "\n",
      "En resumen, la canción explora la complejidad de las emociones en una relación, la dificultad para comunicarse y la sensación de vulnerabilidad y confusión que el amor puede provocar.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Primero, aseguramos que tengamos la transcripción cargada\n",
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "\n",
    "# Ahora intentamos usar la transcripción como contexto\n",
    "try:\n",
    "    prompt = f\"\"\"\n",
    "    Contexto: {transcription}\n",
    "    Pregunta: ¿De queé habla esta canción?\n",
    "    \"\"\"\n",
    "\n",
    "    # Realizamos la solicitud al modelo de Gemini\n",
    "    response = genai.GenerativeModel('gemini-1.5-pro').generate_content(prompt)\n",
    "    \n",
    "    # Imprimir la respuesta\n",
    "    print(\"Respuesta de Gemini:\", response.text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670c23f",
   "metadata": {},
   "source": [
    "## División de la transcripción\n",
    "\n",
    "Dado que no podemos usar la transcripción completa como contexto para el modelo, una posible solución es dividir la transcripción en fragmentos más pequeños. Así, podemos invocar el modelo utilizando solo los fragmentos relevantes para responder a una pregunta específica:\n",
    "\n",
    "<img src='images/system2.png' width=\"1200\">\n",
    "\n",
    "Comencemos cargando la transcripción en la memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "553031dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document\n",
      "Text length: 1213 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the transcription file\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Print basic info about the loaded document\n",
    "print(f\"Loaded {len(documents)} document\")\n",
    "print(f\"Text length: {len(documents[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af1708a",
   "metadata": {},
   "source": [
    "Hay muchas maneras de dividir un documento. En este ejemplo, usaremos un divisor simple que divide el documento en fragmentos de tamaño fijo. Consulta [Divisores de texto](https://python.langchain.com/docs/modules/data_connection/document_transformers/) para obtener más información sobre los diferentes enfoques para dividir documentos.\n",
    "\n",
    "A modo de ejemplo, dividiremos la transcripción en fragmentos de 100 caracteres con una superposición de 20 caracteres y mostraremos los primeros fragmentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "003fbfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 15 chunks\n",
      "\n",
      "First three chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Length: 99 characters\n",
      "Content: Every time I think of you I feel shot right through with a ball of blue It's no problem to mine but\n",
      "\n",
      "Chunk 2:\n",
      "Length: 98 characters\n",
      "Content: problem to mine but it's a problem to find Living alive that I can't be behind There's no sense in\n",
      "\n",
      "Chunk 3:\n",
      "Length: 96 characters\n",
      "Content: There's no sense in telling me The wisdom of the fool won't set you free But that's the way that\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# First load the document\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Create a text splitter with chunk size of 100 and overlap of 20 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the chunks\n",
    "print(f\"Split the document into {len(chunks)} chunks\")\n",
    "\n",
    "# Show the first 3 chunks as an example\n",
    "print(\"\\nFirst three chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c52723",
   "metadata": {},
   "source": [
    "Para nuestra aplicación específica, utilizaremos 1000 caracteres en su lugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bdc86c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 2 chunks\n",
      "\n",
      "First chunk:\n",
      "Length: 998 characters\n",
      "Content: Every time I think of you I feel shot right through with a ball of blue It's no problem to mine but it's a problem to find Living alive that I can't be behind There's no sense in telling me The wisdom of the fool won't set you free But that's the way that goes and it's what nobody knows Well, every day my confusion grows Every time I see all that I get out of my knees and breathe I'm pretty simple, I've been on my own You see the words I can't say I feel fine and I feel good I feel like I'm never sure Wherever I get this way I just don't know what to say Why can't we be ourselves like we were yesterday I'm not sure what this could mean I don't think you'll watch you say I do admit to myself that if I had someone else Then I'll never see it just what I'm not to be Every time I see falling I've been on my own You see the words I can't say Every time I see falling I've been on my own You see the words I can't say Every time I see falling I've been on my own Every time I see falling I've\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter with chunk size of 1000 and overlap of 200 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the chunks\n",
    "print(f\"Split the document into {len(chunks)} chunks\")\n",
    "\n",
    "# Show the first chunk as an example\n",
    "if chunks:\n",
    "    print(\"\\nFirst chunk:\")\n",
    "    print(f\"Length: {len(chunks[0].page_content)} characters\")\n",
    "    print(f\"Content: {chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ec49f",
   "metadata": {},
   "source": [
    "# PARTE III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8f794",
   "metadata": {},
   "source": [
    "## Configuración de un Vector Store\n",
    "\n",
    "Necesitamos una forma eficiente de almacenar fragmentos de documentos, sus Embeddings y realizar búsquedas de similitud a gran escala. Para ello, usaremos un Vector Store.\n",
    "\n",
    "Un Vector Store es una base de datos de Embeddings especializada en búsquedas rápidas de similitud.\n",
    "\n",
    "\n",
    "<img src='images/chain4.png' width=\"1200\">\n",
    "\n",
    "Necesitamos configurar un retriever (https://python.langchain.com/docs/how_to/#retrievers). Este retriever realizará una búsqueda de similitud en el almacén vectorial y devolverá los documentos más similares al siguiente paso de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af54dd",
   "metadata": {},
   "source": [
    "## Configurar Pinecone\n",
    "\n",
    "Para este ejemplo, usaremos [Pinecone](https://www.pinecone.io/).\n",
    "\n",
    "<img src=\"images/pinecone.png\" width=\"800\">\n",
    "\n",
    "El primer paso es crear una cuenta de Pinecone, configurar un índice, obtener una clave API y configurarla como variable de entorno `PINECONE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb82458e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new index 'rag-transcription' with dimension 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71694/1861014624.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0f16bd6f594d4eaf9c1977f35f47e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21c234df69549e5993a58dffcc43315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd8fb1c950344768b6a231a9e42b1ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5a0223a4054538b3e1d53c841e54ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27588bfcb46d42d7b6a76220f0f8e63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c47b94b00f24eaf9958a6394d9835d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9f22bb083a4649bf76fc126bc8166e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce536c6879004e83ae685248b3ad471b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5eda92bb2c48f291c9138bd54c42e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3246b2c307a14b689d7eae66f069163a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b4a7ee0754c8d81f57f5bfe9515eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2 chunks into Pinecone index 'rag-transcription'\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Create Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create the index name \n",
    "index_name = \"rag-transcription\"\n",
    "\n",
    "# Si el índice existe, lo eliminamos primero para recrearlo con la dimensión correcta\n",
    "if index_name in pc.list_indexes().names():\n",
    "    print(f\"Eliminating existing index '{index_name}' to recreate with correct dimensions\")\n",
    "    pc.delete_index(index_name)\n",
    "    # Esperar un momento para que la eliminación se complete\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "\n",
    "# Create the index with the correct dimensions\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,  # HuggingFace 'all-MiniLM-L6-v2' embeddings have 384 dimensions\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")\n",
    "print(f\"Created new index '{index_name}' with dimension 384\")\n",
    "\n",
    "# Initialize HuggingFace embeddings model (no API key needed, runs locally)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store and load the documents\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=chunks,  # Use your previously created chunks\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(chunks)} chunks into Pinecone index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d2b3e",
   "metadata": {},
   "source": [
    "Ahora ejecutemos una búsqueda de similitud en pinecone para asegurarnos de que todo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d3a2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: What are the main topics discussed in the video?\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "I've been on my own You see the words I can't say Every time I see falling I've been on my own You see the words I can't say Every time I see falling I've been on my own Every time I see falling I've been on my own You see the words I can't say Every time I see falling I've been on my own You see the words I can't say I do admit to myself that if I had someone else Then I'll never see it just what I'm not to be\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "Every time I think of you I feel shot right through with a ball of blue It's no problem to mine but it's a problem to find Living alive that I can't be behind There's no sense in telling me The wisdom of the fool won't set you free But that's the way that goes and it's what nobody knows Well, every day my confusion grows Every time I see all that I get out of my knees and breathe I'm pretty simple, I've been on my own You see the words I can't say I feel fine and I feel good I feel like I'm never sure Wherever I get this way I just don't know what to say Why can't we be ourselves like we were yesterday I'm not sure what this could mean I don't think you'll watch you say I do admit to myself that if I had someone else Then I'll never see it just what I'm not to be Every time I see falling I've been on my own You see the words I can't say Every time I see falling I've been on my own You see the words I can't say Every time I see falling I've been on my own Every time I see falling I've\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "query = \"What are the main topics discussed in the video?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\nResults for query:\", query)\n",
    "print(\"-\" * 50)\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fe01f",
   "metadata": {},
   "source": [
    "Configuremos la nueva cadena usando Pinecone como almacén vectorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "306d712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta: What are the key points discussed in the video?\n",
      "\n",
      "Respuesta: The provided text seems to be song lyrics, not a video.  The key themes are:\n",
      "\n",
      "* **Intense feelings for someone:**  \"Every time I think of you I feel shot right through with a ball of blue\" suggests a powerful, possibly painful emotional reaction.\n",
      "* **Confusion and uncertainty:**  \"Every day my confusion grows,\" \"I feel like I'm never sure,\" and \"I just don't know what to say\" express a lack of clarity in the relationship and the speaker's own feelings.\n",
      "* **Longing for connection but struggling to express it:** The lyrics hint at difficulty communicating (\"You see the words I can't say\") and a desire for authenticity (\"Why can't we be ourselves like we were yesterday\").\n",
      "* **Solitude and independence:** The repeated phrase \"I've been on my own\" emphasizes the speaker's solitary state.\n",
      "* **Ambivalence about being alone:** While acknowledging their independence, the speaker also admits \"if I had someone else Then I'll never see it just what I'm not to be,\" suggesting a possible fear of commitment or change.\n",
      "\n",
      "\n",
      "It's important to note that without the actual audio/video, this interpretation is based solely on the provided text and may not fully capture the intended meaning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Crear el prompt de Gemini\n",
    "template = \"\"\"\n",
    "Responde a la siguiente pregunta basándote en el contexto proporcionado:\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta:\n",
    "{question}\n",
    "\n",
    "Responde según el contexto proporcionado. Si no puedes encontrar la respuesta, di \"No lo sé\".\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(context, question):\n",
    "    return template.format(context=context, question=question)\n",
    "\n",
    "# Procesar la cadena RAG con Gemini\n",
    "def rag_chain(question):\n",
    "    # Realizar la búsqueda de similitud\n",
    "    relevant_docs = similarity_search(question, k=3)\n",
    "\n",
    "    # Concatenar los fragmentos relevantes para formar el contexto\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "\n",
    "    # Crear el prompt\n",
    "    prompt = create_prompt(context, question)\n",
    "\n",
    "    try:\n",
    "        # Generar la respuesta usando Gemini\n",
    "        response = genai.GenerativeModel('gemini-1.5-pro').generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la generación: {e}\")\n",
    "        return None\n",
    "\n",
    "# Probar la cadena RAG\n",
    "question = \"What are the key points discussed in the video?\"\n",
    "response = rag_chain(question)\n",
    "\n",
    "# Mostrar la respuesta\n",
    "print(\"\\nPregunta:\", question)\n",
    "print(\"\\nRespuesta:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
