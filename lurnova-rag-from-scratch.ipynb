{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG\n",
        "\n",
        "A continuación se muestra una descripción general de alto nivel del sistema que queremos construir:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src='images/img_1.png' width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PARTE I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Empecemos cargando las variables de entorno que necesitamos utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# This is the YouTube video we're going to use.\n",
        "YOUTUBE_VIDEO = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting up the model\n",
        "Definamos el modelo LLM que utilizaremos como parte del flujo de trabajo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probamos el modelo haciendo una pregunta sencilla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El resultado del modelo es una instancia de `AIMessage` que contiene la respuesta. Podemos extraer esta respuesta encadenando el modelo con un analizador de salida [outputParser](https://python.langchain.com/docs/modules/model_io/output_parsers/).\n",
        "\n",
        "Así es como se ve el encadenamiento del modelo con un analizador de salida:\n",
        "\n",
        "<img src='images/chain1.png' width=\"1200\">\n",
        "\n",
        "Para este ejemplo, utilizaremos un `StrOutputParser` simple para extraer la respuesta como una cadena."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Presentamos las plantillas de preguntas\n",
        "\n",
        "Queremos contextualizar el modelo y la pregunta. [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) Son una forma sencilla de definir y reutilizar indicaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Responda la pregunta según el contexto descrito a continuación. Si no puede responder, responda \"No lo sé\".\n",
        "\n",
        "Contexto: {contexto}\n",
        "\n",
        "Pregunta: {pregunta}\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos encadenar el mensaje con el modelo y el analizador de salida.\n",
        "\n",
        "<img src='images/chain2.png' width=\"1200\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combinación de cadenas\n",
        "\n",
        "Podemos combinar diferentes cadenas para crear flujos de trabajo más complejos. Por ejemplo, creemos una segunda cadena que traduzca la respuesta de la primera a otro idioma.\n",
        "\n",
        "Comencemos creando una nueva plantilla de solicitud para la cadena de traducción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Traduce {answer} al {language}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora podemos crear una nueva cadena de traducción que combine el resultado de la primera cadena con la solicitud de traducción.\n",
        "\n",
        "Así es como se ve el nuevo flujo de trabajo:\n",
        "\n",
        "<img src='images/chain3.png' width=\"1200\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from operator import itemgetter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PARTE II"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transcripcion de video de YouTube\n",
        "\n",
        "El contexto que queremos enviar al modelo proviene de un video de YouTube. Descargamos el video y transcribámoslo con [OpenAI's Whisper](https://openai.com/research/whisper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "\n",
        "\n",
        "if not os.path.exists(\"transcription.txt\"):\n",
        "    youtube = YouTube(YOUTUBE_VIDEO)\n",
        "    audio = youtube.streams.filter(only_audio=True).first()\n",
        "\n",
        "    # Cargamos el modelo base. Este no es el modelo más preciso\n",
        "    # pero es rapido.\n",
        "    whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        file = audio.download(output_path=tmpdir)\n",
        "        transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
        "\n",
        "        with open(\"transcription.txt\", \"w\") as file:\n",
        "            file.write(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a leer la transcripción y mostrar los primeros caracteres para asegurarnos de que todo funciona como se espera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"transcription.txt\") as file:\n",
        "    transcription = file.read()\n",
        "\n",
        "transcription[:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usando la transcripción completa como contexto\n",
        "\n",
        "Si intentamos invocar la cadena usando la transcripción como contexto, el modelo devolverá un error porque el contexto es demasiado largo.\n",
        "\n",
        "Los modelos de lenguaje grandes admiten tamaños de contexto limitados. El vídeo que estamos usando es demasiado largo para que el modelo lo pueda procesar, por lo que necesitamos buscar una solución diferente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    chain.invoke({\n",
        "        \"context\": transcription,\n",
        "        \"question\": \"¿Es una buena idea leer artículos?\"\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## División de la transcripción\n",
        "\n",
        "Dado que no podemos usar la transcripción completa como contexto para el modelo, una posible solución es dividir la transcripción en fragmentos más pequeños. Así, podemos invocar el modelo utilizando solo los fragmentos relevantes para responder a una pregunta específica:\n",
        "\n",
        "<img src='images/system2.png' width=\"1200\">\n",
        "\n",
        "Comencemos cargando la transcripción en la memoria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hay muchas maneras de dividir un documento. En este ejemplo, usaremos un divisor simple que divide el documento en fragmentos de tamaño fijo. Consulta [Divisores de texto](https://python.langchain.com/docs/modules/data_connection/document_transformers/) para obtener más información sobre los diferentes enfoques para dividir documentos.\n",
        "\n",
        "A modo de ejemplo, dividiremos la transcripción en fragmentos de 100 caracteres con una superposición de 20 caracteres y mostraremos los primeros fragmentos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para nuestra aplicación específica, utilizaremos 1000 caracteres en su lugar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PARTE III"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuración de un Vector Store\n",
        "\n",
        "Necesitamos una forma eficiente de almacenar fragmentos de documentos, sus Embeddings y realizar búsquedas de similitud a gran escala. Para ello, usaremos un Vector Store.\n",
        "\n",
        "Un Vector Store es una base de datos de Embeddings especializada en búsquedas rápidas de similitud.\n",
        "\n",
        "\n",
        "<img src='images/chain4.png' width=\"1200\">\n",
        "\n",
        "Necesitamos configurar un retriever (https://python.langchain.com/docs/modules/data_connection/retrievers/). Este retriever realizará una búsqueda de similitud en el almacén vectorial y devolverá los documentos más similares al siguiente paso de la cadena."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configurar Pinecone\n",
        "\n",
        "Para este ejemplo, usaremos [Pinecone](https://www.pinecone.io/).\n",
        "\n",
        "<img src=\"images/pinecone.png\" width=\"800\">\n",
        "\n",
        "El primer paso es crear una cuenta de Pinecone, configurar un índice, obtener una clave API y configurarla como variable de entorno `PINECONE_API_KEY`.\n",
        "\n",
        "Después, podemos cargar los documentos de transcripción en Pinecone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora ejecutemos una búsqueda de similitud en pinecone para asegurarnos de que todo funciona:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuremos la nueva cadena usando Pinecone como almacén vectorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
