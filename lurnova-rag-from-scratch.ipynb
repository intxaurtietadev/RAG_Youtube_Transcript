{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0298f0",
   "metadata": {},
   "source": [
    "# RAG\n",
    "\n",
    "A continuación se muestra una descripción general de alto nivel del sistema que queremos construir:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2552ec",
   "metadata": {},
   "source": [
    "<img src='images/img_1.png' width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53998c7",
   "metadata": {},
   "source": [
    "# PARTE I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5a2950",
   "metadata": {},
   "source": [
    "Empecemos cargando las variables de entorno que necesitamos utilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75600f",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "Definamos el modelo LLM que utilizaremos como parte del flujo de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bdd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Este es el video de YouTube que vamos a utilizar.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=F8NKVhkZZWI&t=1s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18966b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9295f5d",
   "metadata": {},
   "source": [
    "Probamos el modelo haciendo una pregunta sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66f31bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de Túnez es Túnez.\n"
     ]
    }
   ],
   "source": [
    "pregunta_sencilla = \"¿Cuál es la capital de Túnez?\"\n",
    "respuesta = model.invoke(pregunta_sencilla)\n",
    "\n",
    "# Imprimimos el contenido de la respuesta\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10476a6",
   "metadata": {},
   "source": [
    "El resultado del modelo es una instancia de `AIMessage` que contiene la respuesta. Podemos extraer esta respuesta encadenando el modelo con un analizador de salida [outputParser](https://python.langchain.com/docs/modules/model_io/output_parsers/).\n",
    "\n",
    "Así es como se ve el encadenamiento del modelo con un analizador de salida:\n",
    "\n",
    "<img src='images/chain1.png' width=\"1200\">\n",
    "\n",
    "Para este ejemplo, utilizaremos un `StrOutputParser` simple para extraer la respuesta como una cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3bb28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La capital de Alemania es Berlín.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "#(convertir AIMessage a string)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Creamos la cadena simple combinando el modelo y el parser\n",
    "chain = model | parser\n",
    "\n",
    "# Probamos la cadena con la misma pregunta sencilla\n",
    "pregunta_sencilla = \"¿Cuál es la capital de Alemaña?\"\n",
    "respuesta_parseada = chain.invoke(pregunta_sencilla)\n",
    "\n",
    "# Imprimimos la respuesta (ahora debería ser una cadena directamente)\n",
    "print(respuesta_parseada)\n",
    "print(type(respuesta_parseada)) # Para verificar que es un string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b69df2",
   "metadata": {},
   "source": [
    "## Presentamos las plantillas de preguntas\n",
    "\n",
    "Queremos contextualizar el modelo y la pregunta. [Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/quick_start) Son una forma sencilla de definir y reutilizar indicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b985655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Responda la pregunta según el contexto descrito a continuación. Si no puede responder, responda \"No lo sé\".\n",
    "\n",
    "Contexto: {contexto}\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6f25e",
   "metadata": {},
   "source": [
    "Ahora podemos encadenar el mensaje con el modelo y el analizador de salida.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afcc626d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m  RunnablePassthrough\n\u001b[32m      3\u001b[39m chain = (\n\u001b[32m      4\u001b[39m          {\n\u001b[32m      5\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontexto\u001b[39m\u001b[33m\"\u001b[39m: RunnablePassthrough(),\n\u001b[32m      6\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpregunta\u001b[39m\u001b[33m\"\u001b[39m: RunnablePassthrough()\n\u001b[32m      7\u001b[39m         }\n\u001b[32m      8\u001b[39m         | ChatPromptTemplate.from_template(template)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         | \u001b[43mmodel\u001b[49m\n\u001b[32m     10\u001b[39m         | parser\n\u001b[32m     11\u001b[39m     )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import  RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "         {\n",
    "            \"contexto\": RunnablePassthrough(),\n",
    "            \"pregunta\": RunnablePassthrough()\n",
    "        }\n",
    "        | ChatPromptTemplate.from_template(template)\n",
    "        | model\n",
    "        | parser\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87a9e3b",
   "metadata": {},
   "source": [
    "## Combinación de cadenas\n",
    "\n",
    "Podemos combinar diferentes cadenas para crear flujos de trabajo más complejos. Por ejemplo, creemos una segunda cadena que traduzca la respuesta de la primera a otro idioma.\n",
    "\n",
    "Comencemos creando una nueva plantilla de solicitud para la cadena de traducción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45d6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Traduce {answer} al {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2dfdc0",
   "metadata": {},
   "source": [
    "Ahora podemos crear una nueva cadena de traducción que combine el resultado de la primera cadena con la solicitud de traducción.\n",
    "\n",
    "Así es como se ve el nuevo flujo de trabajo:\n",
    "\n",
    "<img src='images/chain3.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f42e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta traducida: Translate {'answer': 'The capital of France is Paris.', 'language': 'Spanish'} to English\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Cadena de respuesta usando contexto (primera cadena, ya definida anteriormente)\n",
    "qa_chain = (\n",
    "    {\n",
    "        \"contexto\": RunnablePassthrough(),\n",
    "        \"pregunta\": RunnablePassthrough()\n",
    "    }\n",
    "    | ChatPromptTemplate.from_template(template)\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Crear la cadena de traducción\n",
    "translation_chain = (\n",
    "    {\n",
    "        \"answer\": RunnablePassthrough(),\n",
    "        \"language\": lambda _: \"Inglés\"  # valor predeterminado\n",
    "    }\n",
    "    | translation_prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Combinamos ambas cadenas: primero obtenemos la respuesta, luego la traducimos\n",
    "combined_chain = qa_chain | (lambda answer: translation_chain.invoke({\"answer\": answer, \"language\": \"Castellano\"}))\n",
    "\n",
    "# Probemos la cadena combinada\n",
    "respuesta_traducida = combined_chain.invoke({\n",
    "    \"contexto\": \"París es la capital de Francia y una de las ciudades más visitadas del mundo.\",\n",
    "    \"pregunta\": \"¿Cuál es la capital de Francia?\"\n",
    "})\n",
    "\n",
    "print(\"Respuesta traducida:\", respuesta_traducida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a07114",
   "metadata": {},
   "source": [
    "# PARTE II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54506480",
   "metadata": {},
   "source": [
    "## Transcripcion de video de YouTube\n",
    "\n",
    "El contexto que queremos enviar al modelo proviene de un video de YouTube. Descargamos el video y transcribámoslo con [OpenAI's Whisper](https://openai.com/research/whisper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a529c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription file already exists!\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import whisper\n",
    "import os\n",
    "import yt_dlp  # Using yt-dlp instead of pytube\n",
    "\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    print(f\"Descargando video: {YOUTUBE_VIDEO}\")\n",
    "    \n",
    "    # Create a temporary directory for the download\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # yt-dlp options for downloading audio only\n",
    "        ydl_opts = {\n",
    "            'format': 'bestaudio/best',\n",
    "            'outtmpl': os.path.join(tmpdir, 'audio.%(ext)s'),\n",
    "            'postprocessors': [{\n",
    "                'key': 'FFmpegExtractAudio',\n",
    "                'preferredcodec': 'mp3',\n",
    "                'preferredquality': '192',\n",
    "            }],\n",
    "            'quiet': False\n",
    "        }\n",
    "        \n",
    "        # Download the audio\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.extract_info(YOUTUBE_VIDEO, download=True)\n",
    "            audio_file = os.path.join(tmpdir, 'audio.mp3')\n",
    "        \n",
    "        print(f\"Transcribiendo archivo de audio: {audio_file}\")\n",
    "        \n",
    "        # Load Whisper model\n",
    "        whisper_model = whisper.load_model(\"base\")\n",
    "        \n",
    "        # Transcribir el audio\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        # Save the transcription to a file\n",
    "        with open(\"transcription.txt\", \"w\") as file:\n",
    "            file.write(transcription)\n",
    "        \n",
    "        print(\"Transcription completed and saved to 'transcription.txt'\")\n",
    "else:\n",
    "    print(\"Transcription file already exists!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3498f1f7",
   "metadata": {},
   "source": [
    "Vamos a leer la transcripción y mostrar los primeros caracteres para asegurarnos de que todo funciona como se espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f854290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Este vídeo está patrocinado por el podcast, cuidado con las macros ocultas de 480. Cuidado con las m'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "transcription[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef4f2de",
   "metadata": {},
   "source": [
    "## Usando la transcripción completa como contexto\n",
    "\n",
    "Si intentamos invocar la cadena usando la transcripción como contexto, el modelo devolverá un error porque el contexto es demasiado largo.\n",
    "\n",
    "Los modelos de lenguaje grandes admiten tamaños de contexto limitados. El vídeo que estamos usando es demasiado largo para que el modelo lo pueda procesar, por lo que necesitamos buscar una solución diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "730f1253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero, aseguramos que tengamos la transcripción cargada\n",
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "\n",
    "# Ahora intentamos usar la transcripción como contexto\n",
    "try:\n",
    "    chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"¿Es una buena idea leer artículos?\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a601df",
   "metadata": {},
   "source": [
    "## División de la transcripción\n",
    "\n",
    "Dado que no podemos usar la transcripción completa como contexto para el modelo, una posible solución es dividir la transcripción en fragmentos más pequeños. Así, podemos invocar el modelo utilizando solo los fragmentos relevantes para responder a una pregunta específica:\n",
    "\n",
    "<img src='images/system2.png' width=\"1200\">\n",
    "\n",
    "Comencemos cargando la transcripción en la memoria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a827d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document\n",
      "Text length: 20821 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the transcription file\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Print basic info about the loaded document\n",
    "print(f\"Loaded {len(documents)} document\")\n",
    "print(f\"Text length: {len(documents[0].page_content)} characters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496bf84",
   "metadata": {},
   "source": [
    "Hay muchas maneras de dividir un documento. En este ejemplo, usaremos un divisor simple que divide el documento en fragmentos de tamaño fijo. Consulta [Divisores de texto](https://python.langchain.com/docs/modules/data_connection/document_transformers/) para obtener más información sobre los diferentes enfoques para dividir documentos.\n",
    "\n",
    "A modo de ejemplo, dividiremos la transcripción en fragmentos de 100 caracteres con una superposición de 20 caracteres y mostraremos los primeros fragmentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8273e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 262 chunks\n",
      "\n",
      "First three chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Length: 98 characters\n",
      "Content: Este vídeo está patrocinado por el podcast, cuidado con las macros ocultas de 480. Cuidado con las\n",
      "\n",
      "Chunk 2:\n",
      "Length: 92 characters\n",
      "Content: Cuidado con las macros ocultas, un podcast de 480. Veamos, seguramente estarás al día de los\n",
      "\n",
      "Chunk 3:\n",
      "Length: 96 characters\n",
      "Content: al día de los increíbles avances que hemos conseguido en el campo del Diplarme durante la última\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# First load the document\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Create a text splitter with chunk size of 100 and overlap of 20 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the chunks\n",
    "print(f\"Split the document into {len(chunks)} chunks\")\n",
    "\n",
    "# Show the first 3 chunks as an example\n",
    "print(\"\\nFirst three chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee1e2d",
   "metadata": {},
   "source": [
    "Para nuestra aplicación específica, utilizaremos 1000 caracteres en su lugar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d146e8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the document into 26 chunks\n",
      "\n",
      "First chunk:\n",
      "Length: 999 characters\n",
      "Content: Este vídeo está patrocinado por el podcast, cuidado con las macros ocultas de 480. Cuidado con las macros ocultas, un podcast de 480. Veamos, seguramente estarás al día de los increíbles avances que hemos conseguido en el campo del Diplarme durante la última década, un montón de cosas. En los comienzos usábamos arquitectura de redes neuronales sencillas como las redes neuronales multicapas para construir los primeros modelos que aprendieron a resolver tareas básicas. Y luego lo fuimos adaptando y perfeccionando a la naturaleza de los diferentes tipos de datos que usábamos. Redes neuronales convolucionales para entender datos espaciales como imágenes. Redes neuronales recurrentes para entender los datos secuenciales como textos. Redes neuronales que no solo se utilizaban para aprender a analizar patrones, sino que también eran capaces de generarlos con resultados que todos vosotros habéis podido ver aquí en el canal. Y si bien el desarrollo de esta tecnología en tan poco tiempo ha sido\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter with chunk size of 1000 and overlap of 200 characters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Display information about the chunks\n",
    "print(f\"Split the document into {len(chunks)} chunks\")\n",
    "\n",
    "# Show the first chunk as an example\n",
    "if chunks:\n",
    "    print(\"\\nFirst chunk:\")\n",
    "    print(f\"Length: {len(chunks[0].page_content)} characters\")\n",
    "    print(f\"Content: {chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdf521",
   "metadata": {},
   "source": [
    "# PARTE III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976473ab",
   "metadata": {},
   "source": [
    "## Configuración de un Vector Store\n",
    "\n",
    "Necesitamos una forma eficiente de almacenar fragmentos de documentos, sus Embeddings y realizar búsquedas de similitud a gran escala. Para ello, usaremos un Vector Store.\n",
    "\n",
    "Un Vector Store es una base de datos de Embeddings especializada en búsquedas rápidas de similitud.\n",
    "\n",
    "\n",
    "<img src='images/chain4.png' width=\"1200\">\n",
    "\n",
    "Necesitamos configurar un retriever (https://python.langchain.com/docs/how_to/#retrievers). Este retriever realizará una búsqueda de similitud en el almacén vectorial y devolverá los documentos más similares al siguiente paso de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82928dae",
   "metadata": {},
   "source": [
    "## Configurar Pinecone\n",
    "\n",
    "Para este ejemplo, usaremos [Pinecone](https://www.pinecone.io/).\n",
    "\n",
    "<img src=\"images/pinecone.png\" width=\"800\">\n",
    "\n",
    "El primer paso es crear una cuenta de Pinecone, configurar un índice, obtener una clave API y configurarla como variable de entorno `PINECONE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87c32146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminating existing index 'rag-transcription' to recreate with correct dimensions\n",
      "Created new index 'rag-transcription' with dimension 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fl/0425_ksd7gl2jnqxhr8qwwhw0000gn/T/ipykernel_8349/1861014624.py:36: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 26 chunks into Pinecone index 'rag-transcription'\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# Initialize Pinecone\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Create Pinecone client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Create the index name \n",
    "index_name = \"rag-transcription\"\n",
    "\n",
    "# Si el índice existe, lo eliminamos primero para recrearlo con la dimensión correcta\n",
    "if index_name in pc.list_indexes().names():\n",
    "    print(f\"Eliminating existing index '{index_name}' to recreate with correct dimensions\")\n",
    "    pc.delete_index(index_name)\n",
    "    # Esperar un momento para que la eliminación se complete\n",
    "    import time\n",
    "    time.sleep(5)\n",
    "\n",
    "# Create the index with the correct dimensions\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,  # HuggingFace 'all-MiniLM-L6-v2' embeddings have 384 dimensions\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")\n",
    "print(f\"Created new index '{index_name}' with dimension 384\")\n",
    "\n",
    "# Initialize HuggingFace embeddings model (no API key needed, runs locally)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create the vector store and load the documents\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=chunks,  # Use your previously created chunks\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(f\"Successfully loaded {len(chunks)} chunks into Pinecone index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64851ef4",
   "metadata": {},
   "source": [
    "Ahora ejecutemos una búsqueda de similitud en pinecone para asegurarnos de que todo funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff0c8ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for query: What are the main topics discussed in the video?\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 1:\n",
      "espacio multidimensional sea muy parecida, representarán a conceptos cuyas palabras también sean parecidas, y se lejarán de aquellas palabras que poco tengan que ver. Matemáticamente, este ángulo, además, lo podríamos calcular para así estudiar pues cuál es la similitud entre palabras, frases o documentos. Si queréis conocer más en detalles sobre esto, recomiendo ver estos dos vídeos de aquí que dan comienzo a la serie de NLP. Pero por ahora para este vídeo me vale con que entendáis que aquí cada palabra viene representada por uno de estos vectores matemáticos. Y con ellos, vamos a trabajar. Porque aquí nuestro objetivo es buscar una solución a este problema de falta de memoria que parece estar presentada en las redes neuronales recurrentes. Esa falta de conexión que parece que existe entre palabras que están muy distanciadas, que no nos permiten estudiar cuáles son sus relaciones. Veamos por ejemplo con esta frase de aquí. El pangolin duerme en su árbol. Aquí nuestro objetivo será\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "que un jugador del dotasia con el ratonio en teclado. Como vimos en el video test, la una día de hoy encontramos proyectos que se siguen basando en este tipo de redes neuronales recurrentes. Y parecería que todo es maravilloso y ideal con este tipo de redes neuronales recurrentes excepto por un pequeño detalle, y es que este video no se titula las redes neuronales recurrentes son maravillosas e ideales. Quiero que me respondas una pregunta. ¿Cuál ha sido la primera palabra que he pronunciado al comienzo de este video? Si no te acuerdas, no pasa nada, y es que es normal que después de haber escupido tantas palabras durante el video, pues tú no haya sido capaz de retenerla. Pero ¿y si te dijera que este problema no es exclusivo de tu limitado cerebro de primate? No. ¿y si te dijera que este es el principal problema al que se enfrentan las redes neuronales recurrentes? Y es que está comprobado que uno de los principales problemas de este tipo de redes es que cuando este proceso de nutrir\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 3:\n",
      "Este vídeo está patrocinado por el podcast, cuidado con las macros ocultas de 480. Cuidado con las macros ocultas, un podcast de 480. Veamos, seguramente estarás al día de los increíbles avances que hemos conseguido en el campo del Diplarme durante la última década, un montón de cosas. En los comienzos usábamos arquitectura de redes neuronales sencillas como las redes neuronales multicapas para construir los primeros modelos que aprendieron a resolver tareas básicas. Y luego lo fuimos adaptando y perfeccionando a la naturaleza de los diferentes tipos de datos que usábamos. Redes neuronales convolucionales para entender datos espaciales como imágenes. Redes neuronales recurrentes para entender los datos secuenciales como textos. Redes neuronales que no solo se utilizaban para aprender a analizar patrones, sino que también eran capaces de generarlos con resultados que todos vosotros habéis podido ver aquí en el canal. Y si bien el desarrollo de esta tecnología en tan poco tiempo ha sido\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "query = \"What are the main topics discussed in the video?\"\n",
    "docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\nResults for query:\", query)\n",
    "print(\"-\" * 50)\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1287d",
   "metadata": {},
   "source": [
    "Configuremos la nueva cadena usando Pinecone como almacén vectorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38868885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the key points discussed in the video?\n",
      "\n",
      "Answer: I'm sorry, but there is no context provided to answer the question.\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create the prompt template\n",
    "template = \"\"\"Answer the following question based on the provided context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer the question based on the context provided. If you cannot find the answer in the context, say so.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the RAG chain\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Create the chain components\n",
    "chain = (\n",
    "    RunnableParallel(\n",
    "        context=lambda x: vectorstore.similarity_search(x[\"question\"], k=3),\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "question = \"What are the key points discussed in the video?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "print(\"\\nQuestion:\", question)\n",
    "print(\"\\nAnswer:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
